---
title: "arctic test"
author: "Laura Caron"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Scaling Texts

```{r echo=FALSE, message=FALSE}

#Load Packages

library(readtext)
library(quanteda)
library(dplyr)
library(stringr)


#Load in the data

#This sets the directory where the texts are located
DATA_DIR <- "C:/Users/carolineschauder/Desktop/Downloads/Arctic Speeches"  

#This command reads in all the file names and stores the texts in a tidy dataframe
arctic <- readtext(paste0(DATA_DIR, "/*"))

#This identifies the row names of the dataframe, which are also used in the corpus
#Note that these docnames are not as nice as for the UN Corpus as they vary a bit

row.names(arctic) <- arctic$doc_id

#This command reads in the corpus based on the filenames we defined above
arcticcorpus <- corpus(arctic, text_field = "text") 



```

The next step is to create a dfm. We can do this in one step with tokenizing.

Unlike in our previous example, we are also going to look at bigrams. Bigrams are two words that often appear together (and trigrams are collections of three words).


```{r}
#The ngrams=1:2 means that we are using bigrams here (pairs of consecutive words) as well as unigrams. The concatenator is what typically separates two words that go together (here a space).
  # We now have to tell R to remove stopwords here otherwise we will get a lot of stopwords in bigrams, like "of the"
# \\p{P} is unicode for punctuation \p{N} is unicode for numbers, \p{S} is uniciode for symbols and currency: https://www.regular-expressions.info/unicode.html

dfm <- tokens(arcticcorpus) %>%
    tokens_remove("\\p{P}", valuetype = "regex", padding = TRUE) %>%
   tokens_remove("\\p{N}", valuetype = "regex", padding = TRUE) %>%
  tokens_remove("\\p{S}", valuetype = "regex", padding = TRUE) %>%
    tokens_remove(stopwords("english"), padding  = TRUE) %>%
    tokens_ngrams(n = 1:2, concatenator = " ") %>%
    dfm()

#Obviously, using bigrams increases the size of the dfm by a lot. Let's only include words that are used at least 10 times and in at least two documents. That also means we will only get words that appear together regularly.

topfeatures(dfm)

```
Clearly, the stopwords list is imperfect. If you want, you can remove (lists) of words that you think should be removed. Lesson: always inspect your data after each processing step!

```{r}
dfm<-dfm_remove(dfm, c("also", "as well", "arctic", "council", "arctic council"))
topfeatures(dfm)
```


Let's create a wordcloud for our reference documents (chosen in the article for substantive reasons).
```{r, echo=TRUE, message=FALSE}
#Y


textplot_wordcloud(dfm_subset(dfm,doc_id=="EDOCS-4287-v2A-ACMMUS10_FAIRBANKS_2017_Statement_by_Foreign_Minister_of_Russian_Federation_EN.pdf" | doc_id=="EDOCS-4283-v2A-ACMMUUS10_FAIRBANKS_2017_Statement_by_Foreign_Minister_of_Iceland.pdf" ), comparison = TRUE, max.words=75)

textplot_wordcloud(dfm_subset(dfm,doc_id=="EDOCS-4296-v1A-ACMMUS10_FAIRBANKS_2017_Statement_by_US_Secretary_of_State.pdf" | doc_id=="EDOCS-4283-v2A-ACMMUUS10_FAIRBANKS_2017_Statement_by_Foreign_Minister_of_Iceland.pdf" ), comparison = TRUE, max.words=75)

```
OK, now we can estimate the wordscores model
```{r}

refscores <- rep(NA,nrow(dfm))

refscores[str_detect(rownames(dfm), "EDOCS-4287-v2A-ACMMUS10_FAIRBANKS_2017_Statement_by_Foreign_Minister_of_Russian_Federation_EN.pdf")] <- -1
refscores[str_detect(rownames(dfm),"EDOCS-4283-v2A-ACMMUUS10_FAIRBANKS_2017_Statement_by_Foreign_Minister_of_Iceland.pdf")] <- 1

#Wordscore model
ws <- textmodel_wordscores(dfm, refscores, scale="linear", smooth=1)

#Save the predicted scores for countries into a dataframe 
wordscore <- as.data.frame(predict(ws, rescaling="lbg"))

```
Now, you can do all kinds of other analyses with the wordscores (like they did in the paper). We are just going to graph the scores to inspect them.
```{r}

library(ggplot2)

#let's change the hideous variable name generated by R

wordscore$score <- wordscore$`predict(ws, rescaling = "lbg")`

#Save a graph in a pdf document so it will have a nice high resolution

pdf("Dotplot.pdf", width=15) 

ggplot(wordscore, aes(y=reorder(rownames(wordscore), wordscore$score), x=score)) +  geom_point()  

dev.off()

```

